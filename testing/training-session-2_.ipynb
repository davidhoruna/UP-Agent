{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_secret(secret_id, secret_key):\n",
    "    region_name = \"us-east-1\"\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(service_name=\"secretsmanager\", region_name=region_name)\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(SecretId=secret_id)\n",
    "    except ClientError as e:\n",
    "        print(f\"Failed to retrieve secret {secret_id}: {str(e)}\")\n",
    "        raise e\n",
    "    secret = json.loads(get_secret_value_response[\"SecretString\"])\n",
    "    return secret[secret_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_api_key = get_secret(\"askjia-dev\", \"GPT35_API_KEY\")\n",
    "chat_api_endpoint = get_secret(\"askjia-dev\", \"GPT35_API_BASE\")\n",
    "\n",
    "embed_api_key = get_secret(\"askjia-dev\", \"EMBEDDING_API_KEY\")\n",
    "embed_api_endpoint = get_secret(\"askjia-dev\", \"EMBEDDING_API_BASE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# Chat Completion\n",
    "model = AzureChatOpenAI(azure_endpoint=chat_api_endpoint,\n",
    "    api_key=chat_api_key,\n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt-35-turbo-16k\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Embeddings\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=embed_api_endpoint,\n",
    "    api_key=embed_api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valmei14\\AppData\\Local\\Temp\\1\\ipykernel_31208\\3079032970.py:2: LangChainDeprecationWarning: The class `UnstructuredFileLoader` was deprecated in LangChain 0.2.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-unstructured package and should be used instead. To use it run `pip install -U :class:`~langchain-unstructured` and import as `from :class:`~langchain_unstructured import UnstructuredLoader``.\n",
      "  loader = UnstructuredFileLoader(r\"C:\\\\Projects\\\\Multi Agent\\\\Notebooks\\\\Data\\\\TV-POL-02234_v1.0.pdf\")\n"
     ]
    }
   ],
   "source": [
    "## LOAD DOCS\n",
    "loader = UnstructuredFileLoader(r\"C:\\\\Projects\\\\Multi Agent\\\\Notebooks\\\\Data\\\\TV-POL-02234_v1.0.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SPLIT DOCS\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## VECTOR STORE\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE QA CHAIN\n",
    "qa = RetrievalQA.from_chain_type(llm=model, chain_type=\"stuff\", retriever=vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valmei14\\AppData\\Local\\Temp\\1\\ipykernel_31208\\1772282440.py:2: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A judicialização refere-se ao fenômeno em que indivíduos ou grupos recorrem ao sistema judiciário com alegação de que os direitos à saúde, garantidos pela constituição ou por normas legais, não estão sendo respeitados. Isso pode ocorrer, por exemplo, quando há dificuldade de acesso a medicamentos que não estão sendo adequadamente fornecidos pelo sistema público de saúde ou pelos planos de saúde privados. Nesses casos, os pacientes podem optar por entrar com ações judiciais para garantir o acesso aos medicamentos necessários. A judicialização da saúde é um tema complexo e controverso, pois envolve questões legais, éticas e econômicas.\n"
     ]
    }
   ],
   "source": [
    "query = \"Fale sobre Judicialização?\"\n",
    "result = qa({\"query\": query})\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QA Chain to StuffDocumentChain\n",
    "- Nós podemos aprimorar essa chain, passando à ela um prompt especifico\n",
    "- Aqui, estaremos usando o documento que foi carregado nas células passadas para simular um VectorStore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usamos o documents carregados e splitados em 'split' para simular um banco vetorizado na memória, salvando no retriever.\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=splits, embedding=embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(model, prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O Tipo de Acesso Formal refere-se aos casos em que o paciente tem acesso aprovado aos medicamentos através de Pagadores (Operadora de Saúde ou SUS) ou quando o paciente faz o pagamento com recursos próprios. Nesses casos, o centro certificado pode optar por condições comerciais especiais ofertadas em contrato.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\": \"Tipo de Acesso Formal?\"})\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primeiro Agent.\n",
    "- Como primeiro agent, podemos criar um ReAct agent (Reasoning and Action).\n",
    "- Um agent pode conter uma ou mais tools, nesse nosso caso, nosso agent vai possuir apenas a tool que realiza uma chamada para nossa rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_react_agent\n",
    "\n",
    "# Função que chama a chain rag\n",
    "def rag_tool_func(input_text: str) -> str:\n",
    "    response = rag_chain.invoke({\"input\": input_text})\n",
    "    return response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criamos uma tool capaz de chamar a função que utiliza o rag_chain.\n",
    "rag_tool = Tool(\n",
    "    name=\"Document RAG QA\",\n",
    "    func=rag_tool_func,\n",
    "    description=(\n",
    "        \"Use este agente quando a pergunta for sobre informações presentes nos documentos carregados. \"\n",
    "        \"Por exemplo, perguntas como 'O que significa Tipo de Acesso Formal?'.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valmei14\\AppData\\Local\\Temp\\1\\ipykernel_31208\\2337228908.py:12: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 1.0. Use :meth:`~Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc.` instead.\n",
      "  rag_agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Template para o agente RAG\n",
    "rag_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=(\n",
    "        \"Você é um agente que auxilia na resposta a perguntas com base em documentos fornecidos.\\n\"\n",
    "        \"Use as ferramentas disponíveis para obter informações adicionais.\\n\\n\"\n",
    "        \"Pergunta: {input}\\n\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Inicializar o agente RAG\n",
    "rag_agent = initialize_agent(\n",
    "    tools=[rag_tool],\n",
    "    llm=model,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    agent_kwargs={\"prompt\": rag_prompt_template}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valmei14\\AppData\\Local\\Temp\\1\\ipykernel_31208\\3395987160.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  document_response = rag_agent.run(\"O que é 'Tipo de Acesso Formal'?\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI should use the Document RAG QA tool to find the answer in the documents.\n",
      "Action: Document RAG QA\n",
      "Action Input: \"O que é 'Tipo de Acesso Formal'?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mO Tipo de Acesso Formal refere-se aos casos em que o paciente tem acesso aprovado aos medicamentos por meio de pagadores, como operadoras de saúde ou o Sistema Único de Saúde (SUS), ou quando o paciente faz o pagamento com recursos próprios. Nesses casos, o centro certificado pode optar por condições comerciais especiais oferecidas em contrato.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: O Tipo de Acesso Formal refere-se aos casos em que o paciente tem acesso aprovado aos medicamentos por meio de pagadores, como operadoras de saúde ou o Sistema Único de Saúde (SUS), ou quando o paciente faz o pagamento com recursos próprios. Nesses casos, o centro certificado pode optar por condições comerciais especiais oferecidas em contrato.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "O Tipo de Acesso Formal refere-se aos casos em que o paciente tem acesso aprovado aos medicamentos por meio de pagadores, como operadoras de saúde ou o Sistema Único de Saúde (SUS), ou quando o paciente faz o pagamento com recursos próprios. Nesses casos, o centro certificado pode optar por condições comerciais especiais oferecidas em contrato.\n"
     ]
    }
   ],
   "source": [
    "#Fazer uma pergunta baseada nos documentos\n",
    "document_response = rag_agent.run(\"O que é 'Tipo de Acesso Formal'?\")\n",
    "print(document_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chamadas diretas\n",
    "- Para chamadas que não necessáriamente precisam de conteúdos pré carregados para serem respondidos, podemos usar uma LLMChain usando o modelo direto, com um simples prompt formato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valmei14\\AppData\\Local\\Temp\\1\\ipykernel_31208\\905444249.py:12: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  model_chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "# Template para o agente do modelo\n",
    "model_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=(\n",
    "        \"Você é um modelo treinado para responder a perguntas com base no conhecimento adquirido durante seu treinamento.\\n\"\n",
    "        \"Responda de forma clara e objetiva.\\n\\n\"\n",
    "        \"Pergunta: {input}\\n\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cadeia direta com o modelo e o prompt\n",
    "model_chain = LLMChain(\n",
    "    llm=model,\n",
    "    prompt=model_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Explique o conceito de aprendizado por reforço.', 'text': 'O aprendizado por reforço é um conceito na área de inteligência artificial e aprendizado de máquina que se baseia em um agente aprender a tomar decisões através de interações com um ambiente. Nesse tipo de aprendizado, o agente recebe feedback em forma de recompensas ou punições, dependendo das ações que ele toma. O objetivo é maximizar as recompensas ao longo do tempo, aprendendo a tomar as melhores decisões em diferentes situações. O agente utiliza técnicas de exploração e exploração para descobrir a melhor estratégia de ação, e o aprendizado ocorre através de tentativa e erro. O aprendizado por reforço é amplamente utilizado em áreas como jogos, robótica e otimização de processos.'}\n"
     ]
    }
   ],
   "source": [
    "# Fazer uma pergunta\n",
    "response = model_chain.invoke(\"Explique o conceito de aprendizado por reforço.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos transformar esse LLMChain ccomo uma tool, encapsulando a chamada .run em uma função model_tool_func\n",
    "\n",
    "def model_tool_func(input_text: str) -> str:\n",
    "    return model_chain.run(input_text)\n",
    "\n",
    "# Primeiro, devemos criar uma tool com a chain. Será nossa model_tool. A tool que responde com informações do treinamento.\n",
    "model_tool = Tool(\n",
    "    name=\"Model Agent\",\n",
    "    func=model_tool_func,\n",
    "    description=(\n",
    "        \"Use este agente quando a pergunta for sobre conhecimento geral, conceitos ou fatos \"\n",
    "        \"aprendidos pelo modelo, como 'O que é aprendizado por reforço?'.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Assim, construimos uma tool que responde ao usuário sem se basear em documentos pré carregados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agent Supervisor\n",
    "- Como sabemos, podemos criar agents com uma ou mais tools.\n",
    "- Cada Tool possui sua descrição, portanto o agent terá conhecimento dos poderes das tools que tem em mãos.\n",
    "- Portanto, pdemos criar um supervisor_agent que possui as duas tools.\n",
    "- Assim, sempre que uma pergunta for realizada para esse agent, ele decidirá quem vai responder a pergunta. Se é a rag_tool ou o model_tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora criamos o agent_supervisor. Ele terá conhecimento das duas tools e será instruido via prompt a responder a pergunta usando a tool correta\n",
    "supervisor_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=(\n",
    "        \"Você é um supervisor que decide qual agente usar para responder a uma pergunta.\\n\"\n",
    "        \"Se a pergunta for sobre informações nos documentos carregados, use o agente RAG.\\n\"\n",
    "        \"Se for uma pergunta geral ou conceitual, use o agente Model.\\n\\n\"\n",
    "        \"Pergunta: {input}\\n\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Inicializar o agente supervisor\n",
    "supervisor_agent = initialize_agent(\n",
    "    tools=[rag_tool, model_tool],\n",
    "    llm=model,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    agent_kwargs={\"prompt\": supervisor_prompt_template}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI'm not familiar with the term \"Carvykti\". I should use the Document RAG QA tool to find information about \"Fluxo de Price Override\".\n",
      "Action: Document RAG QA\n",
      "Action Input: \"Fluxo de Price Override\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mO fluxo de Price Override é um processo que permite a sobreposição ou ajuste manual de preços quando há uma divergência entre o preço cadastrado e o preço negociado para uma transação. Esse processo é utilizado em situações pré-determinadas em que o preço padrão não atende às necessidades específicas da ordem de venda. O time de pricing é responsável por analisar e aprovar as condições comerciais a serem aplicadas em cada Sales Order, de acordo com o contrato vigente, política comercial e tipo de acesso do paciente.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: O fluxo de Price Override é um processo que permite a sobreposição ou ajuste manual de preços quando há uma divergência entre o preço cadastrado e o preço negociado para uma transação. Esse processo é utilizado em situações pré-determinadas em que o preço padrão não atende às necessidades específicas da ordem de venda. O time de pricing é responsável por analisar e aprovar as condições comerciais a serem aplicadas em cada Sales Order, de acordo com o contrato vigente, política comercial e tipo de acesso do paciente.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "O fluxo de Price Override é um processo que permite a sobreposição ou ajuste manual de preços quando há uma divergência entre o preço cadastrado e o preço negociado para uma transação. Esse processo é utilizado em situações pré-determinadas em que o preço padrão não atende às necessidades específicas da ordem de venda. O time de pricing é responsável por analisar e aprovar as condições comerciais a serem aplicadas em cada Sales Order, de acordo com o contrato vigente, política comercial e tipo de acesso do paciente.\n"
     ]
    }
   ],
   "source": [
    "# Agora sempre que uma pergunta for realizada, ele vai verificar se consegue responder usando a rag_tool, caso não consiga, vai responder usando a model_tool.\n",
    "\n",
    "response1 = supervisor_agent.run(\"Fluxo de Price Override para Carvykti?\")\n",
    "print(response1)\n",
    "\n",
    "# response2 = supervisor_agent.run(\"Explique o conceito de aprendizado por reforço.\")\n",
    "# print(\"Resposta do Supervisor (RAG ou Model):\", response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ====================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Como utilizar nossas chains com o langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ainda estamos utilizando a mesma chain criada inicialmente com o retriever. Dessa vez, utilizando a variável {chat_history} que será implementada.\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"Chat History:\\n{chat_history}\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(model, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos um dict representando o estado da aplicação. Esse dict é chamado de State. O state armazena o valor das variáveis que podem/devem ser persistida entre as interações.\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    chat_history: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    context: str\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos então um nó simples que executa o `rag_chain`.\n",
    "# Os valores `return` do nó atualizam o estado do gráfico, então aqui nós apenas atualizamos o histórico de bate-papo com a mensagem de entrada e a resposta.\n",
    "def call_rag_chain(state: State):\n",
    "    response = rag_chain.invoke(state)\n",
    "    return {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(state[\"input\"]),\n",
    "            AIMessage(response[\"answer\"]),\n",
    "        ],\n",
    "        \"context\": response[\"context\"],\n",
    "        \"answer\": response[\"answer\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o fluxo de trabalho\n",
    "workflow = StateGraph(state_schema=State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1f9115a9b90>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adicionamos o ponto de partida e nosso node.\n",
    "workflow.add_edge(START, \"rag\")\n",
    "# Mapeamos que a saida \"rag\" vai executar o node call_rag_chain.\n",
    "workflow.add_node(\"rag\", call_rag_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalmente, compilamos o gráfico com um objeto checkpointer.\n",
    "# Isso persiste o estado, neste caso na memória.\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancelamento é o ato de desistir de uma ordem de venda ou de uma medicação pelo centro certificado. Pode ser solicitado pelo centro certificado através do sistema CCM/CQUENCE e pode ocorrer antes da infusão do produto no paciente. O cancelamento pode ser motivado por razões clínicas, como o óbito do paciente, ou por razões não clínicas, como a desistência do paciente.\n"
     ]
    }
   ],
   "source": [
    "result = app.invoke(\n",
    "    {\"input\": \"O que é cancelamento?\"},\n",
    "    config=config,\n",
    ")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Os motivos clínicos para o cancelamento podem incluir o óbito do paciente ou razões relacionadas à saúde do paciente. Já os motivos não clínicos podem envolver a desistência do paciente. Esses motivos devem ser documentados pelo centro certificado e, caso solicitado pela J&J, o centro deve apresentar a documentação comprobatória do cancelamento.\n"
     ]
    }
   ],
   "source": [
    "result = app.invoke(\n",
    "    {\"input\": \"Fale mais sobre os motivos?\"},\n",
    "    config=config,\n",
    ")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "O que é cancelamento?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Cancelamento é o ato de desistir de uma ordem de venda ou de uma medicação pelo centro certificado. Pode ser solicitado pelo centro certificado através do sistema CCM/CQUENCE e pode ocorrer antes da infusão do produto no paciente. O cancelamento pode ser motivado por razões clínicas, como o óbito do paciente, ou por razões não clínicas, como a desistência do paciente.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Fale mais sobre os motivos?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Os motivos clínicos para o cancelamento podem incluir o óbito do paciente ou razões relacionadas à saúde do paciente. Já os motivos não clínicos podem envolver a desistência do paciente. Esses motivos devem ser documentados pelo centro certificado e, caso solicitado pela J&J, o centro deve apresentar a documentação comprobatória do cancelamento.\n"
     ]
    }
   ],
   "source": [
    "chat_history = app.get_state(config).values[\"chat_history\"]\n",
    "for message in chat_history:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construindo o Graph com o model chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model_chain(state: State):\n",
    "    response = model_chain.invoke(state)\n",
    "    return {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(state[\"input\"]),\n",
    "            AIMessage(response[\"text\"]),\n",
    "        ],\n",
    "        \"answer\": response[\"text\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o fluxo de trabalho\n",
    "workflow = StateGraph(state_schema=State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1f9397476d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adicionamos o ponto de partida e nosso node.\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalmente, compilamos o gráfico com um objeto checkpointer.\n",
    "# Isso persiste o estado, neste caso na memória.\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Michael Jackson foi um cantor, compositor e dançarino norte-americano. Ele é considerado um dos artistas mais influentes e icônicos da história da música pop. Jackson alcançou grande sucesso com álbuns como \"Thriller\" e \"Bad\", e é conhecido por hits como \"Billie Jean\", \"Beat It\" e \"Smooth Criminal\". Ele também foi um inovador em termos de coreografias e videoclipes. Infelizmente, Michael Jackson faleceu em 2009, deixando um legado duradouro na indústria musical.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = app.invoke(\n",
    "    {\"input\": \"Quem foi michael jackson?\"},\n",
    "    config=config,\n",
    ")\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervisor Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ainda estamos utilizando a mesma chain criada inicialmente com o retriever. Dessa vez, utilizando a variável {chat_history} que será implementada.\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"Chat History:\\n{chat_history}\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(model, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazemos o mesmo para a chain que utiliza informações externas, devemos adicionar o histórico.\n",
    "model_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input\", \"chat_history\"],\n",
    "    template=(\n",
    "        \"Você é um modelo treinado para responder a perguntas com base no conhecimento adquirido durante seu treinamento.\\n\"\n",
    "        \"Responda de forma clara e objetiva.\\n\\n\"\n",
    "        \"Utilize o histórico de conversas do usuário para uma precisão melhor nas respostas: {chat_history}\"\n",
    "        \"Pergunta: {input}\\n\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cadeia direta com o modelo e o prompt\n",
    "model_chain = LLMChain(\n",
    "    llm=model,\n",
    "    prompt=model_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criamos novamente nosso state, dessa vez adiccionando mais uma variável 'next'\n",
    "# Essa variável vai armazenar o valor da proxima ação do fluxo seguindo o supervisor.\n",
    "\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    chat_history: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    context: str\n",
    "    answer: str\n",
    "    next: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O supervisor é um nó central no fluxo do LangGraph.\n",
    "# Seu papel é analisar a entrada do usuário e o histórico de mensagens\n",
    "# para decidir qual próximo nó no workflow deve ser acionado.\n",
    "# Ele pode escolher entre:\n",
    "# - \"model\": Para perguntas genéricas que não dependem de informações específicas.\n",
    "# - \"rag\": Para perguntas relacionadas a um conjunto de documentos específicos (neste caso, sobre Carvykti).\n",
    "# A decisão do supervisor é baseada no prompt que combina o input atual do usuário e o histórico do chat.\n",
    "\n",
    "def supervisor(state):\n",
    "    # Obtém o input atual do usuário e o histórico de mensagens do estado.\n",
    "    question = state['input']\n",
    "    messages = state['chat_history']\n",
    "    prompt = f\"\"\"\n",
    "    Você é um agent decisivo em um workflow langgraph.\n",
    "    Seu papel é analisar o input do usuário e definir qual a proxima etapa do fluxo.\n",
    "    Suas opções são:\n",
    "        - model: Responder com o modelo padrão, para respostas genéricas\n",
    "        - rag: Responder com a chain de rag, para respostas relacionadas ao arquivo com informações a respeito da Comercialização de Carvykti.\n",
    "    Sua resposta deve ser APENAS a opção que deve ser usada pelo workflow.\n",
    "    Analise o histórico de mensagens para melhor precisão em sua escolha:\n",
    "    {messages}\n",
    "    Input do usuário:\n",
    "    {question}\n",
    "    Qual node devemos usar, model ou rag?\n",
    "    \"\"\"\n",
    "    # O modelo é chamado com o prompt para decidir a próxima etapa.\n",
    "    supervisor_answer_raw = model.invoke([{\"role\": \"system\", \"content\": prompt}])\n",
    "\n",
    "    # O modelo retorna sua decisão (\"model\" ou \"rag\").\n",
    "    supervisor_answer = supervisor_answer_raw.content.strip()\n",
    "    print(f\"Supervisor escolheu usar:\\n{supervisor_answer}\")\n",
    "    \n",
    "    # Atualiza o estado com a decisão tomada.\n",
    "    state.update({\"next\": supervisor_answer})\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função `validate_subject` é usada para validar a decisão tomada pelo supervisor.\n",
    "# Essa função verifica o valor da chave `next` no estado, que foi atualizado pelo supervisor.\n",
    "# Com base no valor de `next`, ela retorna o nome do próximo nó no workflow.\n",
    "# Se `next` não for válido, a função lança um erro.\n",
    "\n",
    "def validate_subject(state):\n",
    "    try:\n",
    "        # Obtém o próximo nó definido pelo supervisor a partir do estado.\n",
    "        next = state['next']\n",
    "        \n",
    "        # Se o próximo nó for \"model\", retorna o nome do nó \"model\".\n",
    "        if next == \"model\":\n",
    "            return \"model\"\n",
    "        # Se o próximo nó for \"rag\", retorna o nome do nó \"rag\".\n",
    "        elif next == \"rag\":\n",
    "            return \"rag\"\n",
    "    except Exception as e:\n",
    "        # Caso ocorra algum erro (ex.: a chave `next` não existe no estado),\n",
    "        # a função lança uma exceção com uma mensagem explicativa.\n",
    "        raise RuntimeError(\"Erro ao validar o subject\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função `call_model_chain` é responsável por executar o nó que utiliza a chain padrão do modelo.\n",
    "# Ela invoca o `model_chain`, que foi configurado para lidar com perguntas genéricas.\n",
    "# Essa função também atualiza o histórico de mensagens (`chat_history`) no estado, adicionando\n",
    "# a entrada do usuário e a resposta do modelo.\n",
    "\n",
    "def call_model_chain(state: State):\n",
    "    # Invoca a `model_chain` com o estado atual, que contém o input do usuário.\n",
    "    response = model_chain.invoke(state)\n",
    "    \n",
    "    # Retorna um novo estado atualizado, incluindo:\n",
    "    # - O histórico de mensagens atualizado com a entrada do usuário (`HumanMessage`) e a resposta do modelo (`AIMessage`).\n",
    "    # - A resposta gerada pelo modelo no campo `answer`.\n",
    "    return {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(state[\"input\"]),\n",
    "            AIMessage(response[\"text\"]), \n",
    "        ],\n",
    "        \"answer\": response[\"text\"],  \n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função `call_rag_chain` é responsável por executar o nó que utiliza RAG.\n",
    "# Ela invoca a `rag_chain`, que combina recuperação de documentos com geração de respostas.\n",
    "# Essa função também atualiza o histórico de mensagens (`chat_history`) no estado,\n",
    "# além de incluir o contexto retornado pela recuperação.\n",
    "\n",
    "def call_rag_chain(state: State):\n",
    "    # Invoca a `rag_chain` com o estado atual, que contém o input do usuário e possivelmente outras variáveis.\n",
    "    response = rag_chain.invoke(state)\n",
    "\n",
    "    # Retorna um novo estado atualizado, incluindo:\n",
    "    # - O histórico de mensagens atualizado com a entrada do usuário (`HumanMessage`) e a resposta do modelo (`AIMessage`).\n",
    "    # - O contexto recuperado pela RAG no campo `context`.\n",
    "    # - A resposta gerada pela RAG no campo `answer`.\n",
    "    return {\n",
    "        \"chat_history\": [\n",
    "            HumanMessage(state[\"input\"]),  \n",
    "            AIMessage(response[\"answer\"]),  \n",
    "        ],\n",
    "        \"context\": response[\"context\"],\n",
    "        \"answer\": response[\"answer\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o fluxo de trabalho\n",
    "workflow = StateGraph(state_schema=State)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1f939739a90>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adicionamos o ponto de partida e o nó supervisor ao workflow.\n",
    "# O nó \"supervisor\" é responsável por analisar o input do usuário e decidir o próximo passo no fluxo.\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "workflow.add_node(\"supervisor\", supervisor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1f939739a90>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuramos arestas condicionais para o nó \"supervisor\".\n",
    "# O supervisor atualiza o estado com a chave `next`, que pode ser \"rag\" ou \"model\".\n",
    "# Com base no valor de `next`, o workflow segue para o nó correspondente.\n",
    "workflow.add_conditional_edges(\"supervisor\", validate_subject, {\n",
    "    \"rag\": \"rag\",      \n",
    "    \"model\": \"model\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1f939739a90>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adicionamos os nós que representam as chains:\n",
    "workflow.add_node(\"rag\", call_rag_chain)\n",
    "workflow.add_node(\"model\", call_model_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos o checkpoint para persistir o estado do workflow.\n",
    "# Aqui utilizamos o `MemorySaver`, que armazena o estado em memória.\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compilamos o workflow para criar o objeto `app`.\n",
    "# Esse objeto pode ser usado para invocar o fluxo com inputs específicos.\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"configurable\" a chave principal que agrupa todas as configurações personalizáveis para o workflow.\n",
    "# \"thread_id\" representa um identificador único para a execução atual do workflow\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"2\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervisor escolheu usar:\n",
      "model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Resposta: Michael Jackson foi um cantor, compositor e dançarino norte-americano. Ele é considerado um dos artistas mais influentes da história da música pop. Jackson alcançou grande sucesso com álbuns como \"Thriller\" e \"Bad\", e é conhecido por hits como \"Billie Jean\" e \"Beat It\". Ele também foi um dos pioneiros dos videoclipes musicais. Além de sua carreira musical, Jackson também foi um filantropo e ativista pelos direitos humanos. Ele faleceu em 2009.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = app.invoke(\n",
    "    {\"input\": \"Quem foi Michael Jackson?\"},\n",
    "    config=config,\n",
    ")\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervisor escolheu usar:\n",
      "model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Os maiores sucessos de Michael Jackson incluem músicas como \"Thriller\", \"Billie Jean\", \"Beat It\", \"Bad\", entre outros. Ele também alcançou grande sucesso com os álbuns \"Thriller\" e \"Bad\".'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = app.invoke(\n",
    "    {\"input\": \"Quais seus maiores sucessos?\"},\n",
    "    config=config,\n",
    ")\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Quem foi Michael Jackson?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Resposta: Michael Jackson foi um cantor, compositor e dançarino norte-americano. Ele é considerado um dos artistas mais influentes da história da música pop. Jackson alcançou grande sucesso com álbuns como \"Thriller\" e \"Bad\", e é conhecido por hits como \"Billie Jean\" e \"Beat It\". Ele também foi um dos pioneiros dos videoclipes musicais. Além de sua carreira musical, Jackson também foi um filantropo e ativista pelos direitos humanos. Ele faleceu em 2009.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Quais seus maiores sucessos?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Os maiores sucessos de Michael Jackson incluem músicas como \"Thriller\", \"Billie Jean\", \"Beat It\", \"Bad\", entre outros. Ele também alcançou grande sucesso com os álbuns \"Thriller\" e \"Bad\".\n"
     ]
    }
   ],
   "source": [
    "chat_history = app.get_state(config).values[\"chat_history\"]\n",
    "for message in chat_history:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
